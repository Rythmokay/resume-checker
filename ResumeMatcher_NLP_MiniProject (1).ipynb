{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\udde0 Experiment 10: Mini Project Based on NLP Applications\n\n",
        "## \u2728 Project Title: Resume Matcher \u2013 AI-Powered Resume Analysis Tool\n\n",
        "This mini project demonstrates the use of **Natural Language Processing (NLP)** to analyze how well a resume matches a job description.\n\n",
        "### \ud83d\udd0d Key Objectives:\n",
        "- Extract and clean text from resume PDF\n",
        "- Preprocess both resume and job description using NLP techniques\n",
        "- Compare using **cosine similarity** to compute a match score\n",
        "- Identify and visualize top keywords\n",
        "- Provide improvement suggestions based on missing keywords\n\n",
        "This application provides candidates with insights to tailor their resumes for specific job roles more effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udd27 Install & Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# !pip install streamlit PyPDF2 plotly nltk\n",
        "import streamlit as st\n",
        "import PyPDF2\n",
        "import re\n",
        "import io\n",
        "import plotly.graph_objects as go\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udcda Download NLTK Resources\n",
        "We use NLTK's **tokenizers**, **stopwords**, and **lemmatizer** for preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def download_nltk_data():\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "        nltk.data.find('corpora/stopwords')\n",
        "        nltk.data.find('corpora/wordnet')\n",
        "    except LookupError:\n",
        "        nltk.download('punkt')\n",
        "        nltk.download('stopwords')\n",
        "        nltk.download('wordnet')\n",
        "\n",
        "download_nltk_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83e\uddf9 Text Preprocessing\n",
        "We tokenize the text, remove stopwords, and apply lemmatization to normalize the content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def process_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum()]\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udcc4 Extract Text from Resume PDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def extract_text_from_pdf(pdf_file):\n",
        "    try:\n",
        "        with io.BytesIO(pdf_file.read()) as pdf_stream:\n",
        "            pdf_reader = PyPDF2.PdfReader(pdf_stream)\n",
        "            text = \"\"\n",
        "            for page in pdf_reader.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += \" \" + page_text\n",
        "            text = re.sub(r'\\s+', ' ', text)\n",
        "            text = re.sub(r'[^\\w\\s@.,-]', '', text)\n",
        "            return text.lower().strip()\n",
        "    except Exception as e:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83e\udd1d Compare Resume with Job Description\n",
        "We compute cosine similarity between the two term frequency vectors and return the match percentage and keywords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def compare_resume_with_job(resume_text, job_description):\n",
        "    resume_tokens = process_text(resume_text)\n",
        "    job_tokens = process_text(job_description)\n",
        "    resume_freq = Counter(resume_tokens)\n",
        "    job_freq = Counter(job_tokens)\n",
        "    all_terms = set(resume_tokens + job_tokens)\n",
        "    dot_product = sum(resume_freq[term] * job_freq[term] for term in all_terms)\n",
        "    resume_magnitude = math.sqrt(sum(f * f for f in resume_freq.values()))\n",
        "    job_magnitude = math.sqrt(sum(f * f for f in job_freq.values()))\n",
        "    similarity = dot_product / (resume_magnitude * job_magnitude) if resume_magnitude and job_magnitude else 0\n",
        "    return {\n",
        "        'match_percentage': similarity * 100,\n",
        "        'resume_keywords': resume_freq.most_common(10),\n",
        "        'job_keywords': job_freq.most_common(10)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udcca Visualize Keyword Match"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def create_keyword_visualization(results):\n",
        "    all_words = list(set([w for w, _ in results['resume_keywords']] + [w for w, _ in results['job_keywords']]))\n",
        "    resume_dict = dict(results['resume_keywords'])\n",
        "    job_dict = dict(results['job_keywords'])\n",
        "    max_score = max([score for _, score in results['resume_keywords'] + results['job_keywords']] + [1])\n",
        "    resume_scores = [resume_dict.get(word, 0)/max_score * 100 for word in all_words]\n",
        "    job_scores = [job_dict.get(word, 0)/max_score * 100 for word in all_words]\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Bar(x=resume_scores, y=all_words, name='Resume', orientation='h'))\n",
        "    fig.add_trace(go.Bar(x=job_scores, y=all_words, name='Job Description', orientation='h'))\n",
        "    fig.update_layout(title='Keyword Relevance Comparison', xaxis_title='Relevance Score (%)', yaxis_title='Keywords', barmode='group')\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udda5\ufe0f Streamlit App Interface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def main():\n",
        "    st.title(\"\ud83d\udcc4 Resume Matcher \u2013 NLP Based Analysis\")\n",
        "    job_description = st.text_area(\"Paste Job Description here \ud83d\udc47\", height=200)\n",
        "    uploaded_file = st.file_uploader(\"Upload Resume (PDF only)\", type=[\"pdf\"])\n",
        "    if uploaded_file and job_description:\n",
        "        resume_text = extract_text_from_pdf(uploaded_file)\n",
        "        if resume_text:\n",
        "            results = compare_resume_with_job(resume_text, job_description)\n",
        "            st.metric(\"Match Score (%)\", f\"{results['match_percentage']:.2f}\")\n",
        "            st.subheader(\"\ud83d\udd11 Top Keywords Comparison\")\n",
        "            st.plotly_chart(create_keyword_visualization(results), use_container_width=True)\n",
        "            missing = set(w for w, _ in results['job_keywords']) - set(w for w, _ in results['resume_keywords'])\n",
        "            if missing:\n",
        "                st.warning(\"Consider adding these keywords: \" + ', '.join(missing))\n",
        "            else:\n",
        "                st.success(\"Great! Your resume aligns well with the job description!\")\n",
        "        else:\n",
        "            st.error(\"Failed to extract text from PDF.\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \u2705 Conclusion\n\n",
        "This project applies key NLP techniques in a meaningful way to solve a common real-world problem in the job application process.\n\n",
        "**Skills Demonstrated:**\n",
        "- Text preprocessing (tokenization, lemmatization, stopword removal)\n",
        "- Text vectorization and cosine similarity\n",
        "- Data visualization using Plotly\n",
        "- Streamlit web app interface\n\n",
        "**Improvements:**\n",
        "- Highlighting missing keywords directly in the resume text\n",
        "- Adding support for DOCX or image-based resumes\n",
        "- Ranking suggestions for resume rewriting\n\n",
        "This project can be a useful tool for job seekers and recruiters alike."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}