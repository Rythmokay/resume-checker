{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maL5H9eTWYCC"
      },
      "source": [
        "# üß† Experiment 10: Mini Project Based on NLP Applications\n",
        "\n",
        "## ‚ú® Project Title: Resume Matcher ‚Äì AI-Powered Resume Analysis Tool\n",
        "\n",
        "This mini project demonstrates the use of **Natural Language Processing (NLP)** to analyze how well a resume matches a job description.\n",
        "\n",
        "### üîç Key Objectives:\n",
        "- **Extract and clean text** from resume PDF  \n",
        "- **Preprocess** both resume and job description using NLP techniques  \n",
        "- **Compare using cosine similarity** to compute a match score  \n",
        "- **Identify and visualize** top keywords  \n",
        "- **Provide improvement suggestions** based on missing keywords  \n",
        "\n",
        "This application provides candidates with insights to tailor their resumes for specific job roles more effectively.\n"
      ],
      "id": "maL5H9eTWYCC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwZp6SBmWYCE"
      },
      "source": [
        "### üîß Install & Import Required Libraries"
      ],
      "id": "dwZp6SBmWYCE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP04_pn5WYCF"
      },
      "source": [
        "# !pip install streamlit PyPDF2 plotly nltk\n",
        "import streamlit as st\n",
        "import PyPDF2\n",
        "import re\n",
        "import io\n",
        "import plotly.graph_objects as go\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import math"
      ],
      "id": "iP04_pn5WYCF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0IRgT5PWYCG"
      },
      "source": [
        "### üìö Download NLTK Resources\n",
        "We use NLTK's **tokenizers**, **stopwords**, and **lemmatizer** for preprocessing."
      ],
      "id": "i0IRgT5PWYCG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy-T7SqNWYCG"
      },
      "source": [
        "def download_nltk_data():\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "        nltk.data.find('corpora/stopwords')\n",
        "        nltk.data.find('corpora/wordnet')\n",
        "    except LookupError:\n",
        "        nltk.download('punkt')\n",
        "        nltk.download('stopwords')\n",
        "        nltk.download('wordnet')\n",
        "\n",
        "download_nltk_data()"
      ],
      "id": "Hy-T7SqNWYCG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjoMjzDkWYCG"
      },
      "source": [
        "### üßπ Text Preprocessing\n",
        "We tokenize the text, remove stopwords, and apply lemmatization to normalize the content."
      ],
      "id": "mjoMjzDkWYCG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYfniixmWYCH"
      },
      "source": [
        "def process_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum()]\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    return tokens"
      ],
      "id": "rYfniixmWYCH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD_3QO6WWYCH"
      },
      "source": [
        "### üìÑ Extract Text from Resume PDF"
      ],
      "id": "oD_3QO6WWYCH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofv9Mhf2WYCH"
      },
      "source": [
        "def extract_text_from_pdf(pdf_file):\n",
        "    try:\n",
        "        with io.BytesIO(pdf_file.read()) as pdf_stream:\n",
        "            pdf_reader = PyPDF2.PdfReader(pdf_stream)\n",
        "            text = \"\"\n",
        "            for page in pdf_reader.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += \" \" + page_text\n",
        "            text = re.sub(r'\\s+', ' ', text)\n",
        "            text = re.sub(r'[^\\w\\s@.,-]', '', text)\n",
        "            return text.lower().strip()\n",
        "    except Exception as e:\n",
        "        return None"
      ],
      "id": "ofv9Mhf2WYCH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_2xxPAAWYCI"
      },
      "source": [
        "### ü§ù Compare Resume with Job Description\n",
        "We compute cosine similarity between the two term frequency vectors and return the match percentage and keywords."
      ],
      "id": "N_2xxPAAWYCI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CArzLtIVWYCI"
      },
      "source": [
        "def compare_resume_with_job(resume_text, job_description):\n",
        "    resume_tokens = process_text(resume_text)\n",
        "    job_tokens = process_text(job_description)\n",
        "    resume_freq = Counter(resume_tokens)\n",
        "    job_freq = Counter(job_tokens)\n",
        "    all_terms = set(resume_tokens + job_tokens)\n",
        "    dot_product = sum(resume_freq[term] * job_freq[term] for term in all_terms)\n",
        "    resume_magnitude = math.sqrt(sum(f * f for f in resume_freq.values()))\n",
        "    job_magnitude = math.sqrt(sum(f * f for f in job_freq.values()))\n",
        "    similarity = dot_product / (resume_magnitude * job_magnitude) if resume_magnitude and job_magnitude else 0\n",
        "    return {\n",
        "        'match_percentage': similarity * 100,\n",
        "        'resume_keywords': resume_freq.most_common(10),\n",
        "        'job_keywords': job_freq.most_common(10)\n",
        "    }"
      ],
      "id": "CArzLtIVWYCI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqnrdSKFWYCI"
      },
      "source": [
        "### üìä Visualize Keyword Match"
      ],
      "id": "pqnrdSKFWYCI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V77ELXYGWYCI"
      },
      "source": [
        "def create_keyword_visualization(results):\n",
        "    all_words = list(set([w for w, _ in results['resume_keywords']] + [w for w, _ in results['job_keywords']]))\n",
        "    resume_dict = dict(results['resume_keywords'])\n",
        "    job_dict = dict(results['job_keywords'])\n",
        "    max_score = max([score for _, score in results['resume_keywords'] + results['job_keywords']] + [1])\n",
        "    resume_scores = [resume_dict.get(word, 0)/max_score * 100 for word in all_words]\n",
        "    job_scores = [job_dict.get(word, 0)/max_score * 100 for word in all_words]\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Bar(x=resume_scores, y=all_words, name='Resume', orientation='h'))\n",
        "    fig.add_trace(go.Bar(x=job_scores, y=all_words, name='Job Description', orientation='h'))\n",
        "    fig.update_layout(title='Keyword Relevance Comparison', xaxis_title='Relevance Score (%)', yaxis_title='Keywords', barmode='group')\n",
        "    return fig"
      ],
      "id": "V77ELXYGWYCI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odqozXPdWYCJ"
      },
      "source": [
        "### üñ•Ô∏è Streamlit App Interface"
      ],
      "id": "odqozXPdWYCJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFL-pKknWYCJ"
      },
      "source": [
        "def main():\n",
        "    st.title(\"üìÑ Resume Matcher ‚Äì NLP Based Analysis\")\n",
        "    job_description = st.text_area(\"Paste Job Description here üëá\", height=200)\n",
        "    uploaded_file = st.file_uploader(\"Upload Resume (PDF only)\", type=[\"pdf\"])\n",
        "    if uploaded_file and job_description:\n",
        "        resume_text = extract_text_from_pdf(uploaded_file)\n",
        "        if resume_text:\n",
        "            results = compare_resume_with_job(resume_text, job_description)\n",
        "            st.metric(\"Match Score (%)\", f\"{results['match_percentage']:.2f}\")\n",
        "            st.subheader(\"üîë Top Keywords Comparison\")\n",
        "            st.plotly_chart(create_keyword_visualization(results), use_container_width=True)\n",
        "            missing = set(w for w, _ in results['job_keywords']) - set(w for w, _ in results['resume_keywords'])\n",
        "            if missing:\n",
        "                st.warning(\"Consider adding these keywords: \" + ', '.join(missing))\n",
        "            else:\n",
        "                st.success(\"Great! Your resume aligns well with the job description!\")\n",
        "        else:\n",
        "            st.error(\"Failed to extract text from PDF.\")"
      ],
      "id": "tFL-pKknWYCJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgRJ092KWYCJ"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "id": "EgRJ092KWYCJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb9vHuCqWYCJ"
      },
      "source": [
        "### ‚úÖ Conclusion\n",
        "\n",
        "This project applies key NLP techniques in a meaningful way to solve a common real-world problem in the job application process.\n",
        "\n",
        "**Skills Demonstrated:**\n",
        "- Text preprocessing (tokenization, lemmatization, stopword removal)\n",
        "- Text vectorization and cosine similarity\n",
        "- Data visualization using Plotly\n",
        "- Streamlit web app interface\n",
        "\n",
        "**Improvements:**\n",
        "- Highlighting missing keywords directly in the resume text\n",
        "- Adding support for DOCX or image-based resumes\n",
        "- Ranking suggestions for resume rewriting\n",
        "\n",
        "This project can be a useful tool for job seekers and recruiters alike."
      ],
      "id": "Cb9vHuCqWYCJ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}